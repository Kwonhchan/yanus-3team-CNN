{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 리사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 경로\n",
    "fake_image_path = \"Dataset\\fake\"\n",
    "nfake_image_path = \"Dataset\\nfake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(input_path, output_path, new_size):\n",
    "    \"\"\"\n",
    "    이미지를 불러와서 새로운 크기로 리사이즈하는 함수\n",
    "    :param input_path: 원본 이미지 파일 경로\n",
    "    :param output_path: 리사이즈된 이미지를 저장할 파일 경로\n",
    "    :param new_size: 새로운 크기 (너비, 높이) 튜플 형태로 전달\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 이미지 열기\n",
    "        with Image.open(input_path) as img:\n",
    "            # 리사이즈\n",
    "            resized_img = img.resize(new_size)\n",
    "            # 리사이즈된 이미지 저장\n",
    "            resized_img.save(output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_in_directory(directory, output_directory, new_size):\n",
    "    \"\"\"\n",
    "    디렉토리 내에 있는 이미지들을 리사이즈하는 함수\n",
    "    :param directory: 원본 이미지 파일들이 있는 디렉토리 경로\n",
    "    :param output_directory: 리사이즈된 이미지를 저장할 디렉토리 경로\n",
    "    :param new_size: 새로운 크기 (너비, 높이) 튜플 형태로 전달\n",
    "    \"\"\"\n",
    "    # 디렉토리 내의 모든 이미지 파일들을 가져옴\n",
    "    image_files = glob.glob(os.path.join(directory, \"*.jpg\")) + glob.glob(os.path.join(directory, \"*.jpeg\")) + glob.glob(os.path.join(directory, \"*.png\"))\n",
    "\n",
    "    # 출력 디렉토리가 없으면 생성\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # 이미지들을 순회하면서 리사이즈 수행\n",
    "    for image_file in image_files:\n",
    "        filename = os.path.basename(image_file)\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        resize_image(image_file, output_path, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "input_directory = r\"Dataset\\fake\"\n",
    "output_directory = r\"Dataset\\r_fake\"\n",
    "new_size = (224, 224)  # 새로운 크기 (너비, 높이)\n",
    "resize_images_in_directory(input_directory, output_directory, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "input_directory = r\"Dataset\\nfake\"\n",
    "output_directory = r\"Dataset\\r_nfake\"\n",
    "new_size = (224, 224)  # 새로운 크기 (너비, 높이)\n",
    "resize_images_in_directory(input_directory, output_directory, new_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_hls(hls_image, lightness_scale=1.0, saturation_scale=1.0):\n",
    "    h, l, s = cv2.split(hls_image)  # HLS 이미지를 각 채널로 분리\n",
    "    l = np.clip(l * lightness_scale, 0, 255).astype(np.uint8)  # 밝기 조절\n",
    "    s = np.clip(s * saturation_scale, 0, 255).astype(np.uint8)  # 채도 조절\n",
    "    adjusted_hls = cv2.merge([h, l, s])  # 조정된 채널을 다시 합침\n",
    "    return adjusted_hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(folder_paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder_path in folder_paths:\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"경로가 존재하지 않습니다: {folder_path}\")\n",
    "            continue\n",
    "        label = 1 if 'r_fake' in folder_path.lower() else 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            if image_path.endswith('.jpg') or image_path.endswith('.png'):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image_array = np.array(image)[:, :, ::-1]  # RGB to BGR\n",
    "                hls_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2HLS)\n",
    "                adjusted_hls_image = adjust_hls(hls_image, lightness_scale=1.2, saturation_scale=0.9)\n",
    "                # 이미지 정규화 (픽셀 값의 범위를 0 ~ 1 사이로 조정)\n",
    "                normalized_image = adjusted_hls_image / 255.0\n",
    "                images.append(normalized_image)\n",
    "                labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 스크립트가 PS3 폴더 내에 있으므로 상대 경로를 사용합니다.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m folder_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset/r_fake\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset/r_nfake\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 결과 확인\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이미지 개수:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(images))\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mlabeling\u001b[1;34m(folder_paths)\u001b[0m\n\u001b[0;32m     10\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     image_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# RGB to BGR\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     hls_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image_array, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2HLS)\n",
      "File \u001b[1;32mc:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 스크립트가 PS3 폴더 내에 있으므로 상대 경로를 사용합니다.\n",
    "folder_paths = [\"Dataset/r_fake\", \"Dataset/r_nfake\"]\n",
    "images, labels = labeling(folder_paths)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"이미지 개수:\", len(images))\n",
    "print(\"라벨 개수:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        self.images, self.labels = labeling(folder_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # numpy 이미지를 PIL 이미지로 변환합니다. 이 작업이 필요한 이유는 torchvision의 transforms는 PIL 이미지를 기대하기 때문입니다.\n",
    "            image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 사용하기 위한 transform 정의 , RESNET-50(224*224를 입력으로 받음)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),  # PIL 이미지를 PyTorch Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 데이터셋을 파일로 저장하는 함수\n",
    "def save_dataset(dataset, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# 파일에서 데이터셋을 로드하는 함수\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomDataset([\"Dataset/r_fake\", \"Dataset/r_nfake\"], transform=transform)\n",
    "# 데이터셋을 파일로 저장\n",
    "# 저장할 파일명 설정\n",
    "filename = \"dataset.pkl\"\n",
    "save_dataset(full_dataset, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일을 로드하여 데이터셋 사용\n",
    "loaded_dataset = load_dataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomDataset([\"Dataset/r_fake\", \"Dataset/r_nfake\"], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = full_dataset\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = int(len(dataset) * 0.15)\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\kwonh/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 68.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 모델 로드\n",
    "model = resnet50(pretrained=True)\n",
    "# 모델의 마지막 레이어를 이진 분류에 맞게 조정\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)  # 클래스 수를 1로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 268/268 [18:00<00:00,  4.03s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:49<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.0046, Train Acc=0.9991, Val Loss=0.0095, Val Acc=0.9989\n",
      "Validation loss decreased (inf --> 0.009451). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 268/268 [17:20<00:00,  3.88s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:49<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0051, Train Acc=0.9987, Val Loss=0.0032, Val Acc=0.9995\n",
      "Validation loss decreased (0.009451 --> 0.003196). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 268/268 [17:29<00:00,  3.92s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:50<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.0017, Train Acc=0.9994, Val Loss=0.0023, Val Acc=0.9995\n",
      "Validation loss decreased (0.003196 --> 0.002259). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 268/268 [17:29<00:00,  3.91s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:51<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.0042, Train Acc=0.9986, Val Loss=0.0007, Val Acc=1.0000\n",
      "Validation loss decreased (0.002259 --> 0.000663). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 268/268 [17:31<00:00,  3.93s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:50<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.0041, Train Acc=0.9986, Val Loss=0.0116, Val Acc=0.9962\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 268/268 [1:05:27<00:00, 14.66s/it]  \n",
      "Validating: 100%|██████████| 58/58 [00:38<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.0014, Train Acc=0.9992, Val Loss=0.0006, Val Acc=0.9995\n",
      "Validation loss decreased (0.000663 --> 0.000642). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 268/268 [17:14<00:00,  3.86s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:39<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.0031, Train Acc=0.9988, Val Loss=0.0052, Val Acc=0.9978\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 268/268 [17:24<00:00,  3.90s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:40<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.0025, Train Acc=0.9993, Val Loss=0.0010, Val Acc=1.0000\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 268/268 [16:56<00:00,  3.79s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:39<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.0018, Train Acc=0.9994, Val Loss=0.0003, Val Acc=1.0000\n",
      "Validation loss decreased (0.000642 --> 0.000289). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 268/268 [16:53<00:00,  3.78s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:38<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.0003, Train Acc=1.0000, Val Loss=0.0001, Val Acc=1.0000\n",
      "Validation loss decreased (0.000289 --> 0.000148). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 268/268 [16:57<00:00,  3.80s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:41<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.0000, Train Acc=1.0000, Val Loss=0.0001, Val Acc=1.0000\n",
      "Validation loss decreased (0.000148 --> 0.000114). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 268/268 [19:09<00:00,  4.29s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:39<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.0000, Train Acc=1.0000, Val Loss=0.0002, Val Acc=1.0000\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 268/268 [17:25<00:00,  3.90s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:38<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.0025, Train Acc=0.9993, Val Loss=0.0131, Val Acc=0.9951\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 268/268 [17:08<00:00,  3.84s/it]\n",
      "Validating: 100%|██████████| 58/58 [00:39<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.0073, Train Acc=0.9978, Val Loss=0.0022, Val Acc=0.9995\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping initiated. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# 얼리 스톱핑을 위한 초기 설정\n",
    "patience = 3  # 성능 향상이 없는 경우, 몇 에폭 동안 기다릴 것인지\n",
    "val_loss_min = np.Inf  # 가능한 무한대 값으로 초기화\n",
    "patience_counter = 0  # 현재 기다리고 있는 에폭 수\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float().view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) >= 0.5\n",
    "        train_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().view(-1, 1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.sigmoid(outputs) >= 0.5\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # 얼리 스톱핑 조건 검사\n",
    "    if val_loss < val_loss_min:\n",
    "        print(f\"Validation loss decreased ({val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), \"resnet50_binary_classification_model.pth\")\n",
    "        val_loss_min = val_loss\n",
    "        patience_counter = 0  # 리셋\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"EarlyStopping counter: {patience_counter} out of {patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping initiated. Training stopped.\")\n",
    "            break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\kwonh\\Desktop\\ps3\\Dataset\\test\\5.jpg\"\n",
    "\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_array = np.array(image)[:, :, ::-1]  # RGB to BGR\n",
    "hls_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2HLS)\n",
    "adjusted_hls_image = adjust_hls(hls_image, lightness_scale=1.2, saturation_scale=0.9)\n",
    "# 이미지 정규화 (픽셀 값의 범위를 0 ~ 1 사이로 조정)\n",
    "normalized_image = adjusted_hls_image / 255.0\n",
    "image = Image.fromarray((normalized_image * 255).astype(np.uint8))\n",
    "image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 정의\n",
    "model = models.resnet50(pretrained=False)  # 사전 학습된 가중치는 불러오지 않음\n",
    "\n",
    "# 모델의 마지막 레이어를 이진 분류에 맞게 조정 (이진 분류 예시)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합성입니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 상태 로드\n",
    "model.load_state_dict(torch.load(\"resnet50_binary_classification_model.pth\"))\n",
    "\n",
    "input_tensor = image.unsqueeze(0)  # 배치 차원 추가\n",
    "# 모델을 평가 모드로 설정 (예측할 때는 evaluation 모드로 설정)\n",
    "model.eval()\n",
    "with torch.no_grad():  # 추론할 때는 그라디언트를 계산할 필요가 없으므로 no_grad() 컨텍스트 관리자를 사용하여 그라디언트를 비활성화합니다.\n",
    "    outputs = model(input_tensor)\n",
    "    preds = torch.sigmoid(outputs) >= 0.5\n",
    "\n",
    "if preds == True:\n",
    "    print(\"합성입니다.\")\n",
    "else:\n",
    "    print(\"합성이 아닙니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AllLeave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
