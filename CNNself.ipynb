{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 리사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 경로\n",
    "fake_image_path = \"Dataset\\fake\"\n",
    "nfake_image_path = \"Dataset\\nfake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(input_path, output_path, new_size):\n",
    "    \"\"\"\n",
    "    이미지를 불러와서 새로운 크기로 리사이즈하는 함수\n",
    "    :param input_path: 원본 이미지 파일 경로\n",
    "    :param output_path: 리사이즈된 이미지를 저장할 파일 경로\n",
    "    :param new_size: 새로운 크기 (너비, 높이) 튜플 형태로 전달\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 이미지 열기\n",
    "        with Image.open(input_path) as img:\n",
    "            # 리사이즈\n",
    "            resized_img = img.resize(new_size)\n",
    "            # 리사이즈된 이미지 저장\n",
    "            resized_img.save(output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_in_directory(directory, output_directory, new_size):\n",
    "    \"\"\"\n",
    "    디렉토리 내에 있는 이미지들을 리사이즈하는 함수\n",
    "    :param directory: 원본 이미지 파일들이 있는 디렉토리 경로\n",
    "    :param output_directory: 리사이즈된 이미지를 저장할 디렉토리 경로\n",
    "    :param new_size: 새로운 크기 (너비, 높이) 튜플 형태로 전달\n",
    "    \"\"\"\n",
    "    # 디렉토리 내의 모든 이미지 파일들을 가져옴\n",
    "    image_files = glob.glob(os.path.join(directory, \"*.jpg\")) + glob.glob(os.path.join(directory, \"*.jpeg\")) + glob.glob(os.path.join(directory, \"*.png\"))\n",
    "\n",
    "    # 출력 디렉토리가 없으면 생성\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # 이미지들을 순회하면서 리사이즈 수행\n",
    "    for image_file in image_files:\n",
    "        filename = os.path.basename(image_file)\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        resize_image(image_file, output_path, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "input_directory = r\"Dataset\\fake\"\n",
    "output_directory = r\"Dataset\\r_fake\"\n",
    "new_size = (224, 224)  # 새로운 크기 (너비, 높이)\n",
    "resize_images_in_directory(input_directory, output_directory, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = r\"Dataset\\nfake\"\n",
    "output_directory = r\"Dataset\\r_nfake\"\n",
    "new_size = (224, 224)  # 새로운 크기 (너비, 높이)\n",
    "resize_images_in_directory(input_directory, output_directory, new_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_hls(hls_image, lightness_scale=1.0, saturation_scale=1.0):\n",
    "    h, l, s = cv2.split(hls_image)  # HLS 이미지를 각 채널로 분리\n",
    "    l = np.clip(l * lightness_scale, 0, 255).astype(np.uint8)  # 밝기 조절\n",
    "    s = np.clip(s * saturation_scale, 0, 255).astype(np.uint8)  # 채도 조절\n",
    "    adjusted_hls = cv2.merge([h, l, s])  # 조정된 채널을 다시 합침\n",
    "    return adjusted_hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(folder_paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder_path in folder_paths:\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"경로가 존재하지 않습니다: {folder_path}\")\n",
    "            continue\n",
    "        label = 1 if 'r_fake' in folder_path.lower() else 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            if image_path.endswith('.jpg') or image_path.endswith('.png'):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image_array = np.array(image)[:, :, ::-1]  # RGB to BGR\n",
    "                hls_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2HLS)\n",
    "                adjusted_hls_image = adjust_hls(hls_image, lightness_scale=1.2, saturation_scale=0.9)\n",
    "                # 이미지 정규화 (픽셀 값의 범위를 0 ~ 1 사이로 조정)\n",
    "                normalized_image = adjusted_hls_image / 255.0\n",
    "                images.append(normalized_image)\n",
    "                labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 개수: 12229\n",
      "라벨 개수: 12229\n"
     ]
    }
   ],
   "source": [
    "# 스크립트가 PS3 폴더 내에 있으므로 상대 경로를 사용합니다.\n",
    "folder_paths = [\"Dataset/r_fake\", \"Dataset/r_nfake\"]\n",
    "images, labels = labeling(folder_paths)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"이미지 개수:\", len(images))\n",
    "print(\"라벨 개수:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        self.images, self.labels = labeling(folder_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # numpy 이미지를 PIL 이미지로 변환합니다. 이 작업이 필요한 이유는 torchvision의 transforms는 PIL 이미지를 기대하기 때문입니다.\n",
    "            image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 사용하기 위한 transform 정의 , RESNET-50(224*224를 입력으로 받음)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),  # PIL 이미지를 PyTorch Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 데이터셋을 파일로 저장하는 함수\n",
    "def save_dataset(dataset, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# 파일에서 데이터셋을 로드하는 함수\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomDataset([\"Dataset/r_fake\", \"Dataset/r_nfake\"], transform=transform)\n",
    "# 데이터셋을 파일로 저장\n",
    "# 저장할 파일명 설정\n",
    "filename = \"dataset.pkl\"\n",
    "save_dataset(full_dataset, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일을 로드하여 데이터셋 사용\n",
    "loaded_dataset = load_dataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomDataset([\"Dataset/r_fake\", \"Dataset/r_nfake\"], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = full_dataset\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = int(len(dataset) * 0.15)\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=3, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)  # 수정된 크기\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.batch_norm4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 512 * 2 * 2)  # 수정된 view 호출\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = BinaryModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 모델 학습 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 268/268 [00:58<00:00,  4.56it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:14<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.0831, Train Acc=0.5012, Val Loss=0.1173, Val Acc=0.4937\n",
      "Validation loss decreased (inf --> 0.117344). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 268/268 [01:00<00:00,  4.46it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:13<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0340, Train Acc=0.5012, Val Loss=0.6927, Val Acc=0.4937\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 268/268 [00:40<00:00,  6.65it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:10<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.0243, Train Acc=0.5012, Val Loss=0.0593, Val Acc=0.4937\n",
      "Validation loss decreased (0.117344 --> 0.059308). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 268/268 [00:41<00:00,  6.43it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:13<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.0243, Train Acc=0.5012, Val Loss=0.0128, Val Acc=0.4937\n",
      "Validation loss decreased (0.059308 --> 0.012816). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 268/268 [00:44<00:00,  6.08it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:12<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.0156, Train Acc=0.5012, Val Loss=0.0074, Val Acc=0.4937\n",
      "Validation loss decreased (0.012816 --> 0.007436). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 268/268 [00:43<00:00,  6.16it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:11<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.0112, Train Acc=0.5012, Val Loss=0.0307, Val Acc=0.4937\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 268/268 [00:45<00:00,  5.87it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:07<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.0121, Train Acc=0.5012, Val Loss=0.0177, Val Acc=0.4937\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 268/268 [00:41<00:00,  6.49it/s]\n",
      "Validating: 100%|██████████| 58/58 [00:07<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.0127, Train Acc=0.5012, Val Loss=0.0137, Val Acc=0.4937\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping initiated. Training stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 얼리 스톱핑을 위한 초기 설정\n",
    "patience = 3  # 성능 향상이 없는 경우, 몇 에폭 동안 기다릴 것인지\n",
    "val_loss_min = np.Inf  # 가능한 무한대 값으로 초기화\n",
    "patience_counter = 0  # 현재 기다리고 있는 에폭 수\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float().view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) >= 0.5\n",
    "        train_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().view(-1, 1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.sigmoid(outputs) >= 0.5\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # 얼리 스톱핑 조건 검사\n",
    "    if val_loss < val_loss_min:\n",
    "        print(f\"Validation loss decreased ({val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), \"selfmodel_binary_classification_model.pth\")\n",
    "        val_loss_min = val_loss\n",
    "        patience_counter = 0  # 리셋\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"EarlyStopping counter: {patience_counter} out of {patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping initiated. Training stopped.\")\n",
    "            break\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AllLeave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
