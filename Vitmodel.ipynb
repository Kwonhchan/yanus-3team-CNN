{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 리사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 경로\n",
    "fake_image_path = \"Dataset\\fake\"\n",
    "nfake_image_path = \"Dataset\\nfake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(input_path, output_path, new_size):\n",
    "    \"\"\"\n",
    "    이미지를 불러와서 새로운 크기로 리사이즈하는 함수\n",
    "    :param input_path: 원본 이미지 파일 경로\n",
    "    :param output_path: 리사이즈된 이미지를 저장할 파일 경로\n",
    "    :param new_size: 새로운 크기 (너비, 높이) 튜플 형태로 전달\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 이미지 열기\n",
    "        with Image.open(input_path) as img:\n",
    "            # 리사이즈\n",
    "            resized_img = img.resize(new_size)\n",
    "            # 리사이즈된 이미지 저장\n",
    "            resized_img.save(output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_in_directory(directory, output_directory, new_size):\n",
    "    \"\"\"\n",
    "    디렉토리 내에 있는 이미지들을 리사이즈하는 함수\n",
    "    :param directory: 원본 이미지 파일들이 있는 디렉토리 경로\n",
    "    :param output_directory: 리사이즈된 이미지를 저장할 디렉토리 경로\n",
    "    :param new_size: 새로운 크기 (너비, 높이) 튜플 형태로 전달\n",
    "    \"\"\"\n",
    "    # 디렉토리 내의 모든 이미지 파일들을 가져옴\n",
    "    image_files = glob.glob(os.path.join(directory, \"*.jpg\")) + glob.glob(os.path.join(directory, \"*.jpeg\")) + glob.glob(os.path.join(directory, \"*.png\"))\n",
    "\n",
    "    # 출력 디렉토리가 없으면 생성\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # 이미지들을 순회하면서 리사이즈 수행\n",
    "    for image_file in image_files:\n",
    "        filename = os.path.basename(image_file)\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        resize_image(image_file, output_path, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "input_directory = r\"Dataset\\fake\"\n",
    "output_directory = r\"Dataset\\r_fake\"\n",
    "new_size = (224, 224)  # 새로운 크기 (너비, 높이)\n",
    "resize_images_in_directory(input_directory, output_directory, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = r\"Dataset\\nfake\"\n",
    "output_directory = r\"Dataset\\r_nfake\"\n",
    "new_size = (224, 224)  # 새로운 크기 (너비, 높이)\n",
    "resize_images_in_directory(input_directory, output_directory, new_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_hls(hls_image, lightness_scale=1.0, saturation_scale=1.0):\n",
    "    h, l, s = cv2.split(hls_image)  # HLS 이미지를 각 채널로 분리\n",
    "    l = np.clip(l * lightness_scale, 0, 255).astype(np.uint8)  # 밝기 조절\n",
    "    s = np.clip(s * saturation_scale, 0, 255).astype(np.uint8)  # 채도 조절\n",
    "    adjusted_hls = cv2.merge([h, l, s])  # 조정된 채널을 다시 합침\n",
    "    return adjusted_hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(folder_paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder_path in folder_paths:\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"경로가 존재하지 않습니다: {folder_path}\")\n",
    "            continue\n",
    "        label = 1 if 'r_fake' in folder_path.lower() else 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            if image_path.endswith('.jpg') or image_path.endswith('.png'):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image_array = np.array(image)[:, :, ::-1]  # RGB to BGR\n",
    "                hls_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2HLS)\n",
    "                adjusted_hls_image = adjust_hls(hls_image, lightness_scale=1.2, saturation_scale=0.9)\n",
    "                # 이미지 정규화 (픽셀 값의 범위를 0 ~ 1 사이로 조정)\n",
    "                normalized_image = adjusted_hls_image / 255.0\n",
    "                images.append(normalized_image)\n",
    "                labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 개수: 12229\n",
      "라벨 개수: 12229\n"
     ]
    }
   ],
   "source": [
    "# 스크립트가 PS3 폴더 내에 있으므로 상대 경로를 사용합니다.\n",
    "folder_paths = [\"Dataset/r_fake\", \"Dataset/r_nfake\"]\n",
    "images, labels = labeling(folder_paths)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"이미지 개수:\", len(images))\n",
    "print(\"라벨 개수:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        self.images, self.labels = labeling(folder_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # numpy 이미지를 PIL 이미지로 변환합니다. 이 작업이 필요한 이유는 torchvision의 transforms는 PIL 이미지를 기대하기 때문입니다.\n",
    "            image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 사용하기 위한 transform 정의 , RESNET-50(224*224를 입력으로 받음)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),  # PIL 이미지를 PyTorch Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 데이터셋을 파일로 저장하는 함수\n",
    "def save_dataset(dataset, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# 파일에서 데이터셋을 로드하는 함수\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomDataset([\"Dataset/r_fake\", \"Dataset/r_nfake\"], transform=transform)\n",
    "# 데이터셋을 파일로 저장\n",
    "# 저장할 파일명 설정\n",
    "filename = \"dataset.pkl\"\n",
    "save_dataset(full_dataset, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일을 로드하여 데이터셋 사용\n",
    "loaded_dataset = load_dataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomDataset([\"Dataset/r_fake\", \"Dataset/r_nfake\"], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = full_dataset\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = int(len(dataset) * 0.15)\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DataLoader 설정\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def initialize_model():\n",
    "    model_id = \"google/vit-base-patch16-224\"\n",
    "    model = ViTForImageClassification.from_pretrained(model_id)\n",
    "\n",
    "    # 이진 분류를 위해 출력 레이어 수정\n",
    "    model.classifier = torch.nn.Linear(model.config.hidden_size, 2)\n",
    "    \n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)\n",
    "    \n",
    "    return model, feature_extractor\n",
    "\n",
    "model, feature_extractor = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 모델 학습 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 268/268 [37:12<00:00,  8.33s/it]\n",
      "Validating: 100%|██████████| 58/58 [02:00<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.0867, Train Acc=0.9996, Val Loss=0.0554, Val Acc=0.9967\n",
      "Validation loss decreased (inf --> 0.055354). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 268/268 [47:15<00:00, 10.58s/it]\n",
      "Validating: 100%|██████████| 58/58 [03:27<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0842, Train Acc=1.0001, Val Loss=0.0596, Val Acc=1.0000\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 268/268 [1:01:26<00:00, 13.75s/it]\n",
      "Validating: 100%|██████████| 58/58 [03:28<00:00,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.1528, Train Acc=0.9994, Val Loss=0.1012, Val Acc=1.0000\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping initiated. Training stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 얼리 스톱핑을 위한 초기 설정\n",
    "patience = 2  # 성능 향상이 없는 경우, 몇 에폭 동안 기다릴 것인지\n",
    "val_loss_min = np.Inf  # 가능한 무한대 값으로 초기화\n",
    "patience_counter = 0  # 현재 기다리고 있는 에폭 수\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)  # outputs.logits 사용\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs.logits) >= 0.5  # outputs.logits 사용\n",
    "        train_corrects += (preds == labels.unsqueeze(1)).float().sum()  # 수정된 정확도 계산\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_corrects / len(train_loader.dataset)  # double 대신 바로 계산\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)  # outputs.logits 사용\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.sigmoid(outputs.logits) >= 0.5  # outputs.logits 사용\n",
    "            val_corrects += (preds == labels.unsqueeze(1)).float().sum()  # 수정된 정확도 계산\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_corrects / len(val_loader.dataset)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # 얼리 스톱핑 조건 검사\n",
    "    if val_loss < val_loss_min:\n",
    "        print(f\"Validation loss decreased ({val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), \"ViT_classification_model.pth\")\n",
    "        val_loss_min = val_loss\n",
    "        patience_counter = 0  # 리셋\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"EarlyStopping counter: {patience_counter} out of {patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping initiated. Training stopped.\")\n",
    "            break\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\kwonh\\Desktop\\ps3\\Dataset\\test\\3.jpg\"\n",
    "\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_array = np.array(image)[:, :, ::-1]  # RGB to BGR\n",
    "hls_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2HLS)\n",
    "adjusted_hls_image = adjust_hls(hls_image, lightness_scale=1.2, saturation_scale=0.9)\n",
    "# 이미지 정규화 (픽셀 값의 범위를 0 ~ 1 사이로 조정)\n",
    "normalized_image = adjusted_hls_image / 255.0\n",
    "image = Image.fromarray((normalized_image * 255).astype(np.uint8))\n",
    "image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 1: 합성입니다.\n",
      "이미지 2: 합성이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 상태 로드\n",
    "from tqdm import tqdm\n",
    "model.load_state_dict(torch.load(\"ViT_classification_model.pth\"))\n",
    "\n",
    "input_tensor = image.unsqueeze(0)  # 배치 차원 추가\n",
    "# 모델을 평가 모드로 설정 (예측할 때는 evaluation 모드로 설정)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    inputs = input_tensor\n",
    "            \n",
    "    outputs = model(inputs)\n",
    "    preds = torch.sigmoid(outputs.logits) >= 0.5\n",
    "            \n",
    "\n",
    "# 결과 출력 (여러 요소에 대한 처리)\n",
    "for idx, pred in enumerate(preds.squeeze()):  # squeeze()를 사용하여 불필요한 차원 제거\n",
    "    if pred:\n",
    "        print(f\"이미지 {idx+1}: 합성입니다.\")\n",
    "    else:\n",
    "        print(f\"이미지 {idx+1}: 합성이 아닙니다.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AllLeave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
